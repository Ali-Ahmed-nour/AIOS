# Global Configuration for AIOS

# API Keys Configuration
api_keys:
  openai: "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"    # OpenAI API key
  gemini: "AIzaSyD6MNF7_fyndo_Z1jAFErhR4ZOsZ-I2iv4"    # Google Gemini API key
  groq: "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"      # Groq API key
  anthropic: "" # Anthropic API key
  # novita: ""    # Novita AI API key
  huggingface:
    auth_token: "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"  # Your HuggingFace auth token for authorized models
    cache_dir: "/content/drive/MyDrive/AIOS/huggingface_cache"   # Your cache directory for saving huggingface models

# LLM Configuration
llms:
  models:

    # OpenAI Models
    - name: "gpt-4o"
      backend: "openai"

    - name: "gpt-4o-mini"
      backend: "openai"

    # Google Models
    - name: "gemini-2.0-flash"
      backend: "google"


    # Anthropic Models
    # - name: "claude-3-opus"
    #   backend: "anthropic"

    # Ollama Models
    - name: "qwen2.5:72b"
      backend: "ollama"
      hostname: "http://localhost:8091" # Make sure to run ollama server

    # HuggingFace Models
     - name: "meta-llama/Llama-3.1-8B-Instruct"
       backend: "huggingface"
       max_gpu_memory: {0: "15GB"}  # GPU memory allocation
       eval_device: "cpu"  # Device for model evaluation
    
    # vLLM Models
    # To use vllm as backend, you need to install vllm and run the vllm server: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html
    # An example command to run the vllm server is:
    # vllm serve meta-llama/Llama-3.2-3B-Instruct --port 8091
    # - name: "meta-llama/Llama-3.1-8B-Instruct"
    #  backend: "vllm"
    #  hostname: "http://localhost:8091"

    # SGLang Models
    # To use sglang as backend, you need to install sglang and run the sglang server: https://docs.sglang.ai/backend/openai_api_completions.html
    # python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-72B-Instruct --grammar-backend outlines --tool-call-parser qwen25 --host 127.0.0.1 --port 30001 --tp 4 --disable-custom-all-reduce

    - name: "Qwen/Qwen2.5-72B-Instruct"
      backend: "sglang"
      hostname: "http://localhost:30001/v1"

    # Novita Models
    #- name: "meta-llama/llama-4-scout-17b-16e-instruct"
     # backend: "novita"




  log_mode: "console"
  use_context_manager: TRUE

memory:
  log_mode: "console"
  
storage:
  root_dir: "/content/drive/MyDrive/AIOS/root"
  use_vector_db: true

scheduler:
  log_mode: "console"
  strategy: "round_robin"  # أو "fifo" إذا كنت تريد اختبار كلا الاستراتيجيتين

agent_factory:
  log_mode: "console"
  max_workers: 50  
  
server:
  host: "0.0.0.0"
  port: 8000
